{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e596810b",
   "metadata": {},
   "source": [
    "# **nanoGPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb48c8b",
   "metadata": {},
   "source": [
    "## **Important Contents**\n",
    "- [The Bigram Model](#simple-bigram-model)\n",
    "- [The Transformer Architecture](#using-a-transformer-architecture)\n",
    "  - [Implementation](#implementation)\n",
    "  - [Fine Tuning](#fine-tuning)\n",
    "  - [Results](#results)\n",
    "  - [Generation](#generation)\n",
    "  - [Reflection](#reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b415b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo\n",
    "%pip install torch\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3aa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53fe9267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-12 01:21:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2026-01-12 01:21:55 (20.5 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fac8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484b02f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0d0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "print(''.join(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338e338",
   "metadata": {},
   "source": [
    "### **Encodings**\n",
    "We will simply represent each character in our vocabulary as the index of that character in our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16dd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# For individual characters\n",
    "stoi = {s: i for i, s in enumerate(vocab)}\n",
    "itos = {i: s for i, s in enumerate(vocab)}\n",
    "\n",
    "# For words\n",
    "encode = lambda x: [stoi[i] for i in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "temp = encode(\"Hello, World!\")\n",
    "print(temp)\n",
    "print(decode(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8442c042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fcd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "split = int(0.9*len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb069a7",
   "metadata": {},
   "source": [
    "### **Time Dimension**\n",
    "The time dimension in transformers refers to the context length we look at for each character. This helps in randomizing the training, but still maintaining some important patterns found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb0dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is 47\n",
      "When input is tensor([18, 47]) the target is 56\n",
      "When input is tensor([18, 47, 56]) the target is 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "  context = x[:t + 1]\n",
    "  target = y[t]\n",
    "  print(f\"When input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af194575",
   "metadata": {},
   "source": [
    "### **Batch Dimension**\n",
    "The training is done in batches as a 3D tensor, which can be efficiently utilized by GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f12a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == \"train\" else val_data\n",
    "  \n",
    "  # Get random indices for the 4 batches\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "\n",
    "  # Get 8 blocks each from those 4 batches\n",
    "  x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "  y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "\n",
    "  # We get a (4, 8) tensor, with batches as columns, and blocks as rows.\n",
    "\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29dc5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) \n",
      " torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(xb.shape, \"\\n\", yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e21090",
   "metadata": {},
   "source": [
    "### **Simple Bigram Model**\n",
    "Now we implmement a very simple bigram model. This model learns the transformation $f: \\{0, 1, \\dots V - 1 \\} \\rightarrow \\R^V.$ To do this, we use a $V \\times V$ lookup table, where $V$ is the size of our vocabulary, and the values of this table are the parameters we learn during training. This is a character level model, and so the vocabulary consists of all unique characters in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b3729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    logits = self.token_embedding_table(idx)\n",
    "    \n",
    "    # logits.shape = B, T, C\n",
    "    # This adds a C dimensions because for for every block of every batch, \n",
    "    #   we predict probability scores for every charcter in our vocab.\n",
    "    # Pytorch's cross-entropy function wants C to be the second dimension in the logits input, so we reshape.\n",
    "    \n",
    "    if (targets is None):\n",
    "      loss = None\n",
    "\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B * T, C)\n",
    "      targets = targets.view(B * T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T)\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, _ = self(idx)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "vocab_size = len(vocab)  \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, _ = m(xb, yb)\n",
    "print(out.shape)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "tokens = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959656b",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab5b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Typical lr is 3e-4, but for small networks we can use a higher lr\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "loss_list = []\n",
    "\n",
    "for steps in range(10000):\n",
    "  xb, yb = get_batch('train')\n",
    "  logits, loss = m(xb, yb)\n",
    "  loss_list.append(loss.item())\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(loss_list)\n",
    "ax1.set_xlabel(\"Steps\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d52ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LUng:\n",
      "j?\n",
      "o f herere s digl y somm d fethal m,\n",
      "KTIN fl tthane abe fls th un,\n",
      "AD3XEr tidsethe n, or d \n"
     ]
    }
   ],
   "source": [
    "tokens = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3a273",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "Since this model is very simple and does not have a lot of learnable parameters, it doesn't do very well. Yet, we can atleast see some structure in the output- it seems to be arranged much like the Shakespeare examples that we fed the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b589d0",
   "metadata": {},
   "source": [
    "## **Using a Transformer Architecture**\n",
    "To improve performance, we will use a transformer architecture. This architecture is much better than a bigram model because it has a lot more learnable paramters, better encodes the relationship between characters in the vocabulary and so has better understanding of context, and also encodes positional context. So, it captures a lot more of the principal features of basic human language, and is able to better replicate it than the bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e128a7",
   "metadata": {},
   "source": [
    "### **Masking Future Values**\n",
    "In order for the training to mimic generation, we need to mask future tokens so that any current token cannot see what will have next. <br>\n",
    "We can do this by simply making a diagonal matrix.\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "    0.56 & 0 & 0\\\\\n",
    "    0.56 & 0.55 & 0\\\\\n",
    "    0.56 & 0.55 & 0.48\\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "We do this by filling in $-\\inf$ in the ${QK^T} \\over {\\sqrt{d_k}}$ matrix so as to create a triangular matrix as shown above. Then after applying softmax, all indices having $- \\infty$ get evaluted to 0.\n",
    "<br><br>\n",
    "In Python, this can be easily achieved by using a few built-in functions from PyTorch. \\\\\n",
    "First, we create a triangular matrix having only 1s and 0s using\n",
    "\n",
    "```python\n",
    "helper = torch.tril(torch.ones(T, T))\n",
    "```\n",
    "\n",
    "Then, we multiply the ${QK^T} \\over {\\sqrt{d_k}}$ matrix with the `helper` matrix from above, and use `masked_fill` to fill in all 0s for $- \\inf$.\n",
    "\n",
    "```python\n",
    "a2 = qktdk.masked_fill(helper == 0, float('inf'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78daad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper = torch.Size([3, 3])\n",
      " a1 = torch.Size([3, 3]), a2 = torch.Size([3, 3])\n",
      "tensor([[4.5600,    inf,    inf],\n",
      "        [4.5600, 4.5600,    inf],\n",
      "        [4.5600, 4.5600, 4.5600]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code cell is an example of masking.\n",
    "\"\"\"\n",
    "\n",
    "EMB_SIZE = 3\n",
    "\n",
    "helper = torch.tril(torch.ones((EMB_SIZE, EMB_SIZE)))\n",
    "a1 = torch.ones((EMB_SIZE, EMB_SIZE)) * 4.56\n",
    "# masking future values\n",
    "a2 = a1.masked_fill(helper == 0, float('inf'))\n",
    "\n",
    "print(f\"helper = {helper.shape}\\n a1 = {a1.shape}, a2 = {a2.shape}\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c9755",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8abb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbd735aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 12 01:22:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0             53W /  400W |     885MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21c7d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "BLOCK_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "EMB_SIZE = 256\n",
    "VOCAB_SIZE = 65 # Fixed\n",
    "NUM_BLOCKS = 6\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4831891",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88acf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, T: int, h: int):\n",
    "    super().__init__()\n",
    "\n",
    "    self.T = T\n",
    "    self.h = h\n",
    "    self.d_k = EMB_SIZE // h\n",
    "\n",
    "    self.W_q = nn.Linear(EMB_SIZE, self.d_k, bias=False)\n",
    "    self.W_k = nn.Linear(EMB_SIZE, self.d_k, bias=False)\n",
    "    self.W_v = nn.Linear(EMB_SIZE, self.d_k, bias=False)\n",
    "\n",
    "    self.encoder_norm1 = nn.LayerNorm(self.d_k, bias=False)\n",
    "    self.encoder_norm2 = nn.LayerNorm(self.d_k, bias=False)\n",
    "    self.decoder_norm1 = nn.LayerNorm(self.d_k, bias=False)\n",
    "    self.decoder_norm2 = nn.LayerNorm(self.d_k, bias=False)\n",
    "    self.decoder_norm3 = nn.LayerNorm(self.d_k, bias=False)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(EMB_SIZE, 4*EMB_SIZE),\n",
    "                          nn.GELU(),\n",
    "                          nn.Linear(4*EMB_SIZE, EMB_SIZE)\n",
    "                        )\n",
    "    \n",
    "    self.norm1 = nn.LayerNorm(EMB_SIZE, device=device)\n",
    "    self.norm2 = nn.LayerNorm(EMB_SIZE, device=device)\n",
    "    self.proj = nn.Linear(h * self.d_k, EMB_SIZE)\n",
    "    \n",
    "  def _self_attention(self, \n",
    "                      Q: torch.Tensor, \n",
    "                      K: torch.Tensor, \n",
    "                      V: torch.Tensor, \n",
    "                      mask: bool = False):\n",
    "    \n",
    "    _, T, _ = Q.shape\n",
    "    a1 = torch.bmm(Q, K.transpose(1, 2)) / ((self.d_k) ** (1/2))\n",
    "\n",
    "    if (mask):\n",
    "      # masking future values\n",
    "      helper = torch.tril(torch.ones((T, T), device=Q.device))\n",
    "      a1 = a1.masked_fill(helper == 0, float('-inf'))\n",
    "\n",
    "    a2 = F.softmax(a1, dim=-1)\n",
    "    attention = torch.bmm(a2, V)\n",
    "    return attention\n",
    "  \n",
    "  def _multi_head_attention(self, \n",
    "                            Q: torch.Tensor, \n",
    "                            K: torch.Tensor, \n",
    "                            V: torch.Tensor, \n",
    "                            mask: bool = False):\n",
    "    \n",
    "    b1 = [self._self_attention(Q, K, V, mask) for h in range(self.h)]\n",
    "    b2 = torch.cat(b1, dim=-1)\n",
    "    mh_attention = self.proj(b2)\n",
    "    return mh_attention\n",
    "  \n",
    "  def _add_and_norm(self, \n",
    "                    x1: torch.Tensor, \n",
    "                    x2: torch.Tensor,\n",
    "                    norm_layer: torch.nn.Module):\n",
    "    \n",
    "    a = x1 + x2\n",
    "    return norm_layer(a)\n",
    "  \n",
    "  \n",
    "  # def encoder(self, \n",
    "  #             x: torch.Tensor):\n",
    "    \n",
    "  #   Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "\n",
    "  #   mh_out = self._multi_head_attention(Q, K, V)\n",
    "  #   add_norm_out1 = self._add_and_norm(x, mh_out)\n",
    "\n",
    "  #   ff_out = self.feed_forward(add_norm_out1)\n",
    "  #   add_norm_out2 = self._add_and_norm(add_norm_out1, ff_out)\n",
    "\n",
    "  #   return add_norm_out2\n",
    "  \n",
    "  def forward(self, \n",
    "              x: torch.Tensor, \n",
    "              encoder_out: torch.Tensor | None = None\n",
    "              ):\n",
    "    \n",
    "    Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "\n",
    "    mh_out1 = self._multi_head_attention(Q, K, V, mask=True)\n",
    "    add_norm_out1 = self._add_and_norm(x, mh_out1, self.norm1) # (B, T, C)\n",
    "\n",
    "    # mh_out2 = self._multi_head_attention(encoder_out, encoder_out, add_norm_out1)\n",
    "    # add_norm_out2 = self._add_and_norm(add_norm_out1, mh_out2)\n",
    "\n",
    "    ff_out = self.feed_forward(add_norm_out1)\n",
    "    add_norm_out3 = self._add_and_norm(add_norm_out1, ff_out, self.norm2)\n",
    "\n",
    "    return add_norm_out3\n",
    "  \n",
    "  \n",
    "class ShakespeareGPT(nn.Module):\n",
    "  def __init__(self, T, h, num_blocks):\n",
    "    super().__init__()\n",
    "\n",
    "    self.T = T\n",
    "    self.h = h\n",
    "\n",
    "    self.get_token_embeddings = nn.Embedding(VOCAB_SIZE, EMB_SIZE)\n",
    "    self.get_pos_embeddings = nn.Embedding(self.T, EMB_SIZE)\n",
    "\n",
    "    self.lm_head = nn.Linear(EMB_SIZE, VOCAB_SIZE)\n",
    "\n",
    "    self.decoder_blocks = nn.ModuleList([\n",
    "      Decoder(T, h) for _ in range(num_blocks)\n",
    "    ])\n",
    "\n",
    "  def forward(self, \n",
    "            X: torch.Tensor, \n",
    "            Y: torch.Tensor | None = None\n",
    "            ):\n",
    "  \n",
    "    loss = None\n",
    "    \n",
    "    B, T = X.shape\n",
    "    \n",
    "    tok_emb = self.get_token_embeddings(X) # x.shape = B, T, C\n",
    "    pos_emb = self.get_pos_embeddings(torch.arange(T, device=X.device))\n",
    "    x = tok_emb + pos_emb\n",
    "\n",
    "    # encoder_out = self.encoder(x)\n",
    "    for decoder in self.decoder_blocks:\n",
    "      x = decoder(x)\n",
    "\n",
    "    logits = self.lm_head(x)\n",
    "    # logits, loss = F.softmax(logits, dim=-1), None\n",
    "\n",
    "    if Y is not None:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      Y = Y.view(B*T)\n",
    "      loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, \n",
    "              idx: list[int], \n",
    "              max_new_tokens: int\n",
    "              ):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "      block_idx = idx[:, -self.T: ]\n",
    "      logits, loss = self(block_idx)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      \n",
    "      # Get index with highest probability\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcad13",
   "metadata": {},
   "source": [
    "### **Fine Tuning**\n",
    "This section is a little out of order, since I ran the model several times using different parameters and I settled with the ones two cells defined above.This gives us a model with 3.9M parameters, and running it for 5000 epochs gives us a loss of about 0.4, which is good enough for me in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dee6a",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "Now it is time to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f506f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == \"train\" else val_data\n",
    "  \n",
    "  # Get random indices for the BATCH_SIZE batches\n",
    "  ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "\n",
    "  # Get BLOCK_SIZE blocks each from those BATCH_SIZE batches\n",
    "  x = torch.stack([data[i: i + BLOCK_SIZE] for i in ix])\n",
    "  y = torch.stack([data[i + 1: i + BLOCK_SIZE + 1] for i in ix])\n",
    "\n",
    "  # We get a (BATCH_SIZE, BLOCK_SIZE) tensor, with batches as columns, and blocks as rows.\n",
    "  x, y = x.to(device), y.to(device)\n",
    "\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ad7b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ShakespeareGPT                           [1, 8, 65]                --\n",
       "├─Embedding: 1-1                         [1, 8, 256]               16,640\n",
       "├─Embedding: 1-2                         [8, 256]                  65,536\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─Decoder: 2-1                      [1, 8, 256]               320\n",
       "│    │    └─Linear: 3-1                  [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-2                  [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-3                  [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-4                  [1, 8, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-5               [1, 8, 256]               512\n",
       "│    │    └─Sequential: 3-6              [1, 8, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-7               [1, 8, 256]               512\n",
       "│    └─Decoder: 2-2                      [1, 8, 256]               320\n",
       "│    │    └─Linear: 3-8                  [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-9                  [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-10                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-11                 [1, 8, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-12              [1, 8, 256]               512\n",
       "│    │    └─Sequential: 3-13             [1, 8, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-14              [1, 8, 256]               512\n",
       "│    └─Decoder: 2-3                      [1, 8, 256]               320\n",
       "│    │    └─Linear: 3-15                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-16                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-17                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-18                 [1, 8, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-19              [1, 8, 256]               512\n",
       "│    │    └─Sequential: 3-20             [1, 8, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-21              [1, 8, 256]               512\n",
       "│    └─Decoder: 2-4                      [1, 8, 256]               320\n",
       "│    │    └─Linear: 3-22                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-23                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-24                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-25                 [1, 8, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-26              [1, 8, 256]               512\n",
       "│    │    └─Sequential: 3-27             [1, 8, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-28              [1, 8, 256]               512\n",
       "│    └─Decoder: 2-5                      [1, 8, 256]               320\n",
       "│    │    └─Linear: 3-29                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-30                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-31                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-32                 [1, 8, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-33              [1, 8, 256]               512\n",
       "│    │    └─Sequential: 3-34             [1, 8, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-35              [1, 8, 256]               512\n",
       "│    └─Decoder: 2-6                      [1, 8, 256]               320\n",
       "│    │    └─Linear: 3-36                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-37                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-38                 [1, 8, 64]                16,384\n",
       "│    │    └─Linear: 3-39                 [1, 8, 256]               65,792\n",
       "│    │    └─LayerNorm: 3-40              [1, 8, 256]               512\n",
       "│    │    └─Sequential: 3-41             [1, 8, 256]               525,568\n",
       "│    │    └─LayerNorm: 3-42              [1, 8, 256]               512\n",
       "├─Linear: 1-4                            [1, 8, 65]                16,705\n",
       "==========================================================================================\n",
       "Total params: 3,950,017\n",
       "Trainable params: 3,950,017\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.41\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.90\n",
       "Params size (MB): 15.79\n",
       "Estimated Total Size (MB): 16.69\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = ShakespeareGPT(T=BLOCK_SIZE, h=4, num_blocks=NUM_BLOCKS)\n",
    "\n",
    "dummy_x = torch.randint(\n",
    "    low=0,\n",
    "    high=VOCAB_SIZE,\n",
    "    size=(1, 8),\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "summary(model, input_data=dummy_x, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab57340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/5000 [00:00<08:05, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.3104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 103/5000 [00:08<06:28, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 2.4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 203/5000 [00:16<06:19, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss: 2.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 303/5000 [00:24<06:12, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Loss: 1.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 403/5000 [00:31<06:03, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400, Loss: 1.6188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 503/5000 [00:39<05:55, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss: 1.5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 603/5000 [00:47<05:51, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600, Loss: 1.4473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 703/5000 [00:55<05:40, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700, Loss: 1.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 803/5000 [01:03<05:31, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800, Loss: 1.3070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 903/5000 [01:11<05:28, 12.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900, Loss: 1.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1003/5000 [01:19<05:15, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 1.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 1103/5000 [01:27<05:07, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100, Loss: 1.2640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 1203/5000 [01:35<05:01, 12.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1200, Loss: 1.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 1303/5000 [01:43<04:50, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300, Loss: 1.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 1403/5000 [01:50<04:45, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400, Loss: 1.1675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 1503/5000 [01:58<04:35, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500, Loss: 1.1493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 1603/5000 [02:06<04:27, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1600, Loss: 1.1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 1703/5000 [02:14<04:19, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1700, Loss: 1.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 1803/5000 [02:22<04:12, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800, Loss: 1.0584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 1903/5000 [02:30<04:06, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1900, Loss: 1.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2003/5000 [02:38<03:57, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, Loss: 1.0280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 2103/5000 [02:46<03:50, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2100, Loss: 0.9584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 2203/5000 [02:54<03:43, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2200, Loss: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 2303/5000 [03:02<03:33, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2300, Loss: 0.9374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 2403/5000 [03:10<03:25, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2400, Loss: 0.9231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 2503/5000 [03:17<03:18, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500, Loss: 0.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 2603/5000 [03:25<03:09, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2600, Loss: 0.8697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2703/5000 [03:33<03:00, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2700, Loss: 0.8559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 2803/5000 [03:41<02:53, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2800, Loss: 0.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 2903/5000 [03:49<02:45, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2900, Loss: 0.7847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3003/5000 [03:57<02:38, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000, Loss: 0.7609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3103/5000 [04:05<02:31, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3100, Loss: 0.7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 3203/5000 [04:13<02:23, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3200, Loss: 0.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 3303/5000 [04:21<02:13, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3300, Loss: 0.7133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 3403/5000 [04:29<02:06, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3400, Loss: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 3503/5000 [04:36<01:59, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3500, Loss: 0.6517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 3603/5000 [04:44<01:50, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3600, Loss: 0.6096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 3703/5000 [04:52<01:42, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3700, Loss: 0.6306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 3803/5000 [05:00<01:35, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3800, Loss: 0.5954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 3903/5000 [05:08<01:26, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3900, Loss: 0.5442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4003/5000 [05:16<01:19, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000, Loss: 0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 4103/5000 [05:24<01:11, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4100, Loss: 0.5276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 4203/5000 [05:32<01:03, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4200, Loss: 0.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 4303/5000 [05:40<00:54, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4300, Loss: 0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 4403/5000 [05:48<00:47, 12.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4400, Loss: 0.4724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 4503/5000 [05:55<00:39, 12.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4500, Loss: 0.4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 4603/5000 [06:03<00:31, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4600, Loss: 0.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 4703/5000 [06:11<00:23, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4700, Loss: 0.4436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 4803/5000 [06:19<00:15, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4800, Loss: 0.4281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 4903/5000 [06:27<00:07, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4900, Loss: 0.4115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [06:35<00:00, 12.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = ShakespeareGPT(T=BLOCK_SIZE, h=4, num_blocks=NUM_BLOCKS)\n",
    "\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in tqdm(range(5000), desc='Training'):\n",
    "  xb, yb = get_batch('train')\n",
    "  logits, loss = model(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  tqdm.write(f\"Epoch {epoch}, Loss: {loss.item():.4f}\") if epoch % 100 == 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab0be0",
   "metadata": {},
   "source": [
    "### **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b73cb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "COMINIUS:\n",
      "O, will dost those badren ourselves,\n",
      "The common vast flood hap to your grace\n",
      "From gibines worship, for the modest moon\n",
      "Upon this hour, marching to command;\n",
      "Only red the rank of this kingly violent.\n",
      "See, how now, which fair deputy,\n",
      "Dightsing fright hay. Usuff into thisding Margares?\n",
      "Are they letter hem mine, their tongues have for ever.\n",
      "\n",
      "KATHARINA:\n",
      "An's we suppose so.\n",
      "3 repare her, I talk of this: I slain\n",
      "Sometime companion blots to serve our hands:\n",
      "O that once cannot be forsworn.\n",
      "Now, break, soft! what news? Warest hast thou with thee?\n",
      "'Tis believe that resign thine own son,\n",
      "Thou counterfeit'st me; this is an end.\n",
      "\n",
      "KING HENRY VI:\n",
      "Ah, know you not when you shall be my son:\n",
      "Hidest grace the away: laid thy anchorse look\n",
      "From my former hands.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Where is the business of the king?\n",
      "\n",
      "PARIS:\n",
      "Monday, my lord.\n",
      "\n",
      "JULIET:\n",
      "What is my office, sir?\n",
      "\n",
      "JULIET:\n",
      "Go thou queate the casafe, and my friend Norfolk\n",
      "Upon some more piece with an agazem. Take good counsellor.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "I do beseech you, look to ask him what this man sword,\n",
      "Should be incense the adminished letters,\n",
      "And believe in heaven and many of your accusations--\n",
      "Learn us me pondallity, and was it beat before must\n",
      "Became of thunder-law.\n",
      "\n",
      "POMPEY:\n",
      "If you should smile he's infection,\n",
      "Is more than spokes your father joys.\n",
      "Why, in this country, and his minim not\n",
      "To Bolingbroke the lamentation of his majesty\n",
      "Which he sends a sweetly deputed\n",
      "That eighty words and shake him lives,\n",
      "And made roaring look on thee. Honour now!\n",
      "\n",
      "CAMILLO:\n",
      "My lord,\n",
      "Shall be true summer, sand my lord remempt\n",
      "Suck her on her! Gembord Clarence, fair lords of this fair land!\n",
      "When Oxford, friends, make war. Whence, men!\n",
      "If thou art smip them to seek a few brat?\n",
      "Ay, like a good friar, surely.\n",
      "Where's my father withholds love come to him.\n",
      "\n",
      "BUCKINGHAM:\n",
      "What, talk you with slow this?\n",
      "\n",
      "DERBY:\n",
      "It is the matter: why, o, then I'll prove him,\n",
      "That stay my appetite, now will in hand.\n",
      "\n",
      "BRUTUS:\n",
      "We stood to't them all.\n",
      "\n",
      "CORIOLANUS:\n",
      "Dire not, s\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# m.generate(context, max_new_tokens=2000)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635267b5",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "I think this is a very good result given the small scale of the project. This model has about 4M parameters, and with further fine-tuning it can do even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
